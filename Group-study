# ================================================================
# Note: Replace placeholders such as <your-username>, <your_account_id>, 
# <your_ecr_image_url>, and <cluster-name> with your own information.
# ================================================================

Step 1: System Update and AWS CLI Installation
===============================================
sudo yum update -y
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
aws --version

===============================================
Step 2: Set up Kubernetes repository
===============================================

cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.31/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.31/rpm/repodata/repomd.xml.key
EOF

==============================================================================
Step 3: Install Docker, Kubernetes CLI, and start services
==============================================================================

sudo yum install -y docker kubectl kubeadm kubelet
sudo systemctl start docker
sudo systemctl enable docker
sudo systemctl enable --now kubelet
sudo yum update -y
sudo systemctl start kubelet
sudo systemctl enable kubelet

==============================================================================
Step 4: Install eksctl
==============================================================================
curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
sudo mv /tmp/eksctl /usr/local/bin
sudo chmod +x /usr/local/bin/eksctl
eksctl version

==============================================================================
Step 5: Create EKS Cluster (Example Command)
==============================================================================
eksctl create cluster --name <cluster-name> --version 1.21 --region us-east-1 --nodegroup-name <nodegroup-name> --node-type t3.medium --nodes 2 --nodes-min 1 --nodes-max 3 --managed

@@@@@@ eksctl create cluster --name trainSchedule-cluster --region us-east-1 --nodegroup-name standard-workers --node-type t2.medium --nodes 2 --managed

==============================================================================
Step 6: Configure kubectl with EKS cluster and check node status
==============================================================================

aws eks --region us-east-1 update-kubeconfig --name <cluster-name>
kubectl get nodes

==============================================================================
Step 7: Clone Git repository and authenticate Docker with AWS ECR
==============================================================================

sudo yum install git -y
git clone https://github.com/<your-username>/cicd-pipeline-train-schedule-kubernetes.git

==============================================================================
Step 8: Build, tag, and push Docker image to ECR
==============================================================================

cd cicd-pipeline-train-schedule-kubernetes/
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin <your_account_id>.dkr.ecr.us-east-1.amazonaws.com
docker build -t trainschedulerepo:latest .
docker tag trainschedulerepo:latest <your_account_id>.dkr.ecr.us-east-1.amazonaws.com/trainschedulerepo:latest
docker push <your_account_id>.dkr.ecr.us-east-1.amazonaws.com/trainschedulerepo:latest



==============================================================================
Step 9: Deploy and expose application with Kubernetes
==============================================================================

# Create and edit deployment.yaml
nano deployment.yaml

# Add the following content:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: train-schedule-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: train-schedule-app
  template:
    metadata:
      labels:
        app: train-schedule-app
    spec:
      containers:
        - name: train-schedule-app
          image: 050752638165.dkr.ecr.us-east-1.amazonaws.com/train_schedulerepo:latest  # Your image
          ports:
            - containerPort: 3000  # The container listens on port 3000


# Apply the deployment
kubectl apply -f deployment.yaml    

# Create and edit service.yaml
nano service.yaml

# Add the following content:
apiVersion: v1
kind: Service
metadata:
  name: train-schedule-service
spec:
  type: NodePort  # Expose using NodePort
  selector:
    app: train-schedule-app  # This should match the label in your deployment
  ports:
    - port: 80            # Expose the service externally on port 80
      targetPort: 3000     # Forward traffic to the container's port 3000
      nodePort: 30080      # Optionally specify a NodePort (auto-assigned by Kubernetes if omitted)
      protocol: TCP

# Apply the service
kubectl apply -f service.yaml

==============================================================================
Step 10: Verify application deployment
==============================================================================
kubectl get service
kubectl get pods
kubectl get pods -o wide
kubectl get pods --all-namespaces
kubectl get pods --all-namespaces -o wide

# End of instructions
### Summary Table:

ports and their uses

# Port Types in Kubernetes
==========================

== Summary Table ==

==========================================================================================================================
| Port Type           | Port Range         | Typical Use Case                                      | Scope               |
|=====================|-===================|-------------------------------------------------------|---------------------|
| ContainerPort       | 0-65535            | The port your container is listening on (e.g., 3000)  | Pod                 |
| Service Port        | 0-65535            | Exposed port for services inside the cluster          | Cluster             |
| Target Port         | 0-65535            | The port on the pod to which the service forwards traffic | Pod             |
| NodePort            | 30000-32767        | Exposed port for services outside the cluster         | Node External       |
| LoadBalancer Port   | 0-65535            | Exposed via external load balancer (e.g., 80/443)     | External (Cloud)    |
| Ingress Port        | 80, 443            | Exposes HTTP/HTTPS traffic via an Ingress controller  | External            |
| ClusterIP Port      | 1-65535            | Default internal service port                         | Cluster             |
| Well-Known Ports    | 1-1023             | Standard network services (e.g., HTTP, HTTPS, DNS)    | Internal/External   |
=========================================================================================================================
==========================

